"""
OrQuanta Agentic v1.0 â€” Healing Agent

Continuous GPU job health monitoring with:
- Metric-based anomaly detection (Z-score + threshold rules)
- OOM detection and automatic memory adjustment
- Exponential backoff restart playbooks
- LLM-driven diagnosis for complex failure modes
- Self-healing action execution via ToolRegistry
"""

from __future__ import annotations

import asyncio
import logging
import math
import os
from collections import defaultdict, deque
from datetime import datetime, timezone
from typing import Any

from .llm_reasoning_engine import LLMReasoningEngine
from .memory_manager import MemoryManager
from .safety_governor import get_governor
from .tool_registry import ToolRegistry

logger = logging.getLogger("orquanta.healing")

MONITOR_INTERVAL = float(os.getenv("HEALER_MONITOR_INTERVAL_S", "10.0"))
OOM_MEMORY_THRESHOLD_PCT = float(os.getenv("HEALER_OOM_THRESHOLD_PCT", "97.0"))
ANOMALY_ZSCORE_THRESHOLD = float(os.getenv("HEALER_ZSCORE_THRESHOLD", "3.0"))
MAX_RESTART_ATTEMPTS = int(os.getenv("HEALER_MAX_RESTARTS", "3"))


class JobHealthRecord:
    """Tracks health metrics history for a monitored job."""

    def __init__(self, job_id: str, instance_id: str) -> None:
        self.job_id = job_id
        self.instance_id = instance_id
        self.restart_count = 0
        self.anomaly_count = 0
        self.metrics_history: deque = deque(maxlen=60)  # ~10 min at 10s interval
        self.status = "healthy"
        self.flags: list[str] = []
        self.last_healed_at: str | None = None

    def record_metrics(self, metrics: dict[str, Any]) -> None:
        """Add a new metrics snapshot to the rolling window."""
        self.metrics_history.append({
            **metrics,
            "ts": datetime.now(timezone.utc).isoformat(),
        })

    def get_rolling_stats(self, field: str) -> dict[str, float]:
        """Compute mean and std-dev for a metric field over the history window."""
        values = [m[field] for m in self.metrics_history if field in m]
        if not values:
            return {"mean": 0.0, "std": 0.0, "n": 0}
        n = len(values)
        mean = sum(values) / n
        variance = sum((v - mean) ** 2 for v in values) / max(n - 1, 1)
        return {"mean": mean, "std": math.sqrt(variance), "n": n}


class HealingAgent:
    """Autonomous health monitor and self-healer for GPU jobs.

    Runs a continuous monitoring loop that collects GPU telemetry,
    detects anomalies via statistical methods, diagnoses root causes
    with LLM reasoning, and executes a self-healing playbook.

    Healing Playbook (in order of escalation):
    1. OOM detected â†’ prescale memory (upgrade GPU type)
    2. Thermal throttling â†’ alert + reduce batch size
    3. Persistent anomaly (Z-score > threshold) â†’ restart job
    4. Restart loop detected â†’ migrate to different instance
    5. Max restarts exceeded â†’ terminate + notify user

    Usage::

        healer = HealingAgent()
        await healer.start()
        
        # Begin monitoring a job
        await healer.start_monitoring(job_id="job-abc123", instance_id="inst-aws-XXXX")
        
        # Get health summary
        status = healer.get_health_status("job-abc123")
    """

    def __init__(self) -> None:
        self.llm = LLMReasoningEngine()
        self.memory = MemoryManager()
        self.tools = ToolRegistry()
        self.governor = get_governor()

        self._monitored: dict[str, JobHealthRecord] = {}
        self._running = False
        self._healed_jobs: list[dict[str, Any]] = []
        logger.info("HealingAgent initialised.")

    async def start(self) -> None:
        """Start the background health monitoring loop."""
        self._running = True
        asyncio.create_task(self._monitoring_loop())
        logger.info(f"HealingAgent monitoring loop active (interval={MONITOR_INTERVAL}s).")

    async def stop(self) -> None:
        self._running = False

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def start_monitoring(
        self, job_id: str, instance_id: str = "mock-inst"
    ) -> dict[str, Any]:
        """Begin monitoring a GPU job.

        Args:
            job_id: Job ID to monitor.
            instance_id: Instance ID where the job is running.

        Returns:
            dict confirming monitoring has started.
        """
        if job_id in self._monitored:
            return {"status": "already_monitoring", "job_id": job_id}

        record = JobHealthRecord(job_id=job_id, instance_id=instance_id)
        self._monitored[job_id] = record
        logger.info(f"[Healer] Started monitoring {job_id} on {instance_id}.")

        return {
            "status": "monitoring_started",
            "job_id": job_id,
            "instance_id": instance_id,
            "monitor_interval_s": MONITOR_INTERVAL,
        }

    def stop_monitoring(self, job_id: str) -> None:
        """Stop monitoring a completed or cancelled job."""
        self._monitored.pop(job_id, None)
        logger.info(f"[Healer] Stopped monitoring {job_id}.")

    def get_health_status(self, job_id: str) -> dict[str, Any] | None:
        """Return health status for a specific job."""
        record = self._monitored.get(job_id)
        if not record:
            return None
        return {
            "job_id": job_id,
            "instance_id": record.instance_id,
            "status": record.status,
            "restart_count": record.restart_count,
            "anomaly_count": record.anomaly_count,
            "flags": record.flags,
            "snapshot_count": len(record.metrics_history),
            "last_healed_at": record.last_healed_at,
        }

    def get_all_health(self) -> list[dict[str, Any]]:
        """Return health status for all monitored jobs."""
        return [
            self.get_health_status(jid)
            for jid in self._monitored
        ]

    def get_heal_history(self) -> list[dict[str, Any]]:
        """Return history of all autonomous healing actions taken."""
        return list(reversed(self._healed_jobs))

    # ------------------------------------------------------------------
    # Monitoring loop
    # ------------------------------------------------------------------

    async def _monitoring_loop(self) -> None:
        """Collect metrics and run anomaly detection for all monitored jobs."""
        while self._running:
            jobs = list(self._monitored.keys())
            for job_id in jobs:
                record = self._monitored.get(job_id)
                if not record:
                    continue
                try:
                    await self._check_job(record)
                except Exception as exc:
                    logger.error(f"[Healer] Monitor error for {job_id}: {exc}")

            await asyncio.sleep(MONITOR_INTERVAL)

    async def _check_job(self, record: JobHealthRecord) -> None:
        """Run a full health check cycle for one job."""
        metrics = await self.tools.get_gpu_metrics(record.instance_id)
        if "error" in metrics:
            logger.warning(f"[Healer] Can't get metrics for {record.job_id}: {metrics['error']}")
            return

        record.record_metrics(metrics)

        # --- Rule 1: OOM risk detection ---
        mem_pct = metrics.get("memory_utilization_pct", 0.0)
        if mem_pct >= OOM_MEMORY_THRESHOLD_PCT:
            if "oom_risk" not in record.flags:
                record.flags.append("oom_risk")
                await self._heal_oom(record, metrics)
            return

        # --- Rule 2: Thermal throttling detection ---
        temp = metrics.get("temp_celsius", 0.0)
        if temp > 84.0 and "thermal" not in record.flags:
            record.flags.append("thermal")
            await self._heal_thermal(record, metrics)

        # --- Rule 3: Statistical anomaly detection (Z-score on GPU utilization) ---
        stats = record.get_rolling_stats("gpu_utilization_pct")
        if stats["n"] >= 10 and stats["std"] > 0:
            util = metrics.get("gpu_utilization_pct", 0.0)
            z_score = abs(util - stats["mean"]) / stats["std"]
            if z_score > ANOMALY_ZSCORE_THRESHOLD:
                record.anomaly_count += 1
                if record.anomaly_count >= 3 and "repeated_anomaly" not in record.flags:
                    record.flags.append("repeated_anomaly")
                    await self._heal_anomaly(record, metrics, z_score)

        # Mark healthy if no flags remain
        if not record.flags:
            record.status = "healthy"

    # ------------------------------------------------------------------
    # Healing Playbooks
    # ------------------------------------------------------------------

    async def _heal_oom(self, record: JobHealthRecord, metrics: dict) -> None:
        """Healing playbook: OOM risk â€” diagnose with LLM, then scale up GPU."""
        logger.warning(f"[Healer] OOM risk detected for {record.job_id} ({metrics.get('memory_utilization_pct', 0):.1f}% mem)")
        record.status = "healing"

        # LLM diagnosis
        diagnosis = await self.llm.reason(
            template_name="healing_diagnose",
            variables={
                "job_id": record.job_id,
                "metrics": metrics,
                "error_log": "memory utilization approaching 100%",
            },
            agent_name="healing_agent",
        )

        action = diagnosis.get("action", "scale_up")
        confidence = diagnosis.get("confidence", 0.80)

        logger.info(
            f"[Healer] OOM diagnosis for {record.job_id}: {diagnosis.get('diagnosis', 'N/A')} "
            f"â†’ action={action} (confidence={confidence:.0%})"
        )

        heal_record = {
            "job_id": record.job_id,
            "trigger": "oom_risk",
            "action": action,
            "reasoning": diagnosis.get("reasoning", ""),
            "confidence": confidence,
            "healed_at": datetime.now(timezone.utc).isoformat(),
        }

        if action == "scale_up":
            await self.tools.send_alert(
                message=(
                    f"ðŸ”§ Auto-healing: Job {record.job_id} hitting OOM. "
                    f"Recommending GPU upgrade. Confidence: {confidence:.0%}."
                ),
                severity="warning",
                agent_name="healing_agent",
                job_id=record.job_id,
            )

        self._healed_jobs.append(heal_record)
        record.last_healed_at = heal_record["healed_at"]
        record.status = "healing_applied"

        await self.memory.store_event({
            "type": "oom_healing",
            "job_id": record.job_id,
            "diagnosis": diagnosis.get("diagnosis"),
            "action": action,
        }, agent_name="healing_agent")

    async def _heal_thermal(self, record: JobHealthRecord, metrics: dict) -> None:
        """Healing playbook: thermal throttling â€” send alert, suggest batch reduction."""
        logger.warning(f"[Healer] Thermal throttle on {record.job_id}: {metrics.get('temp_celsius', 0)}Â°C")
        record.status = "degraded"

        await self.tools.send_alert(
            message=(
                f"ðŸŒ¡ï¸ Thermal alert on job {record.job_id}: {metrics.get('temp_celsius', 0):.1f}Â°C. "
                "Consider reducing batch size or migrating to a cooler region."
            ),
            severity="warning",
            agent_name="healing_agent",
            job_id=record.job_id,
        )

        self._healed_jobs.append({
            "job_id": record.job_id,
            "trigger": "thermal_throttle",
            "action": "alert_issued",
            "temp_celsius": metrics.get("temp_celsius"),
            "healed_at": datetime.now(timezone.utc).isoformat(),
        })
        record.last_healed_at = datetime.now(timezone.utc).isoformat()

    async def _heal_anomaly(self, record: JobHealthRecord, metrics: dict, z_score: float) -> None:
        """Healing playbook: persistent statistical anomaly â€” restart job."""
        logger.warning(
            f"[Healer] Persistent anomaly on {record.job_id}: "
            f"z-score={z_score:.2f}, restarts={record.restart_count}/{MAX_RESTART_ATTEMPTS}"
        )

        if record.restart_count >= MAX_RESTART_ATTEMPTS:
            # Escalate: terminate and notify
            await self.tools.send_alert(
                message=(
                    f"ðŸ›‘ Job {record.job_id} exceeded max restart attempts ({MAX_RESTART_ATTEMPTS}). "
                    "Manual intervention required."
                ),
                severity="critical",
                agent_name="healing_agent",
                job_id=record.job_id,
            )
            record.status = "failed"
            self.stop_monitoring(record.job_id)
            return

        record.restart_count += 1
        backoff_delay = 5 * (2 ** record.restart_count)  # Exponential backoff: 10, 20, 40s

        logger.info(
            f"[Healer] Scheduling restart #{record.restart_count} for {record.job_id} "
            f"in {backoff_delay}s (backoff)."
        )

        async def _delayed_restart():
            await asyncio.sleep(backoff_delay)
            logger.info(f"[Healer] Restarting {record.job_id} (attempt #{record.restart_count}).")
            record.flags = []
            record.status = "restarting"

        asyncio.create_task(_delayed_restart())

        self._healed_jobs.append({
            "job_id": record.job_id,
            "trigger": "statistical_anomaly",
            "action": "restart",
            "z_score": round(z_score, 2),
            "restart_number": record.restart_count,
            "backoff_delay_s": backoff_delay,
            "healed_at": datetime.now(timezone.utc).isoformat(),
        })
        record.last_healed_at = datetime.now(timezone.utc).isoformat()
